Развёртка RNN

Последовательно читаем текст и копим информацию в h

![](Materials/Lect8-1761214016825.jpeg)

Но как обучать?

![](Materials/Lect8-1761214049514.jpeg)

Но градиенты считать тяжело

![](Materials/Lect8-1761214113769.jpeg)

Теперь формулы:

![](Materials/Lect8-1761214179171.jpeg)

в первой строчке:
с помощью chain rule раскрываем дифференцирование от всех зависимых внутри переменных

из-за того, что это рекурсия, то $W_{hh}$ будет встречаться t-1 раз в формуле
![](Materials/Lect8-1761214455089.jpeg)

Поэтому производную не так просто посчитать

поэтому формула бесконечно раскрывается следующим образом
![](Materials/Lect8-1761214533637.jpeg)

В итоге получаем произведение t матриц

когда где-то в формуле позникает 1000-ная степень, то все значения приходят либо в 0, либо большое число (либо взрываться, либо затухать)

![](Materials/Lect8-1761214795961.jpeg)

То есть не пойдет для длинных текстов

Первое решение

LSTM

![](Materials/Lect8-1761214993860.jpeg)

Аналог res_net из-за обходного пути (C_t не проходит умножение), модель может протаскивать некоторую информацию

GRU

![](Materials/Lect8-1761215231259.jpeg)

Мы выполняем все последовательно, хотя в жизни мы смотрим на предложение целиком

![](Materials/Lect8-1761215424202.jpeg)

Bidirectional LSTM

Будем смотреть и на следующее и на предыдущее слово

![](Materials/Lect8-1761215487803.jpeg)

Двунаправленный lstm мы не можем использовать для генерации текста!

### Seq2seq

- Машинный перевод
- Суммаризация текста
- Генерация комментариев к коду
- Математическое преобразования
- Смена стиля текста

![](Materials/Lect8-1761215648183.jpeg)

Что делать, когда длины входного и выходного текстов разные?

Придумали архитектуру Encoder Decoder

![](Materials/Lect8-1761215706091.jpeg)

![](Materials/Lect8-1761215737261.jpeg)

После h_n получаем скрытый вектор (вектор контекста)

Потом декодер на каждом шаге генерирует каждое слово

##### Beam search

![](Materials/Lect8-1761215844113.jpeg)

Порой, когда мы пишем тексты, иногда возвращаемся в назад и редактируем что-то

Давайте будем поддерживать в текущий момент 4 последовательности: запоминаем 4 варианта первого слова, 4 самых вероятных (из 16) второго слова и так далее

EOS - End of sequence

Seq2seq Machine Translation
- четырёхслойные LSTM в качестве кодировщика и декодировщика
- В каждом слове - скрытые векторы размерности 1000
- Каждое слово описывается векторным представлением размерности 1000
- Входной текст подаётся "наоборот" - тогда первое слово входного текста оказывается ближе к первому слову выходного в нашей архитектуре

Проблемы seq2seq архитектуры

Нужно сжать весь текст в один вектор
Теряется информация о первых словах

Механизм внимания

Версия 1:

![](Materials/Lect8-1761216751289.jpeg)

При генерации всех выходов мы используем ...

Не работает

Версия 2

![](Materials/Lect8-1761216906028.jpeg)

Представим, что я генерирую токен номер t

$\alpha_{it}$ - обучаемый параметр

Но мы прибили конеретный входной на конкретный выходной вектор. Размер тот же

Слишком много параметром (t^2), влияния от данных не зависят

Механизм внимания, версия 3

![](Materials/Lect8-1761217089279.jpeg)

проблема: нет нормировки. C_t может получаться жирным. Давайте присобачим сюда софтмакс.

Версия 4

![](Materials/Lect8-1761217322650.jpeg)

Это и есть итоговый механизм внимания, который хорошо работает

![](Materials/Lect8-1761217776110.jpeg)

Кодировщик
- Начнём с качественного прочтения входного текста
- Попробуем обогатить каждое входное слово информацией обо всем тексте
- Назовём это "вниманием на себя"

![](Materials/Lect8-1761218086508.jpeg)

Проблема: мы находили "похожие" слова и подмешали их в вектор.

# Трансформер: 
