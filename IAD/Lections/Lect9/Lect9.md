В прошлой лекции мы поняли, что у рек моделей полно проблем

если кодировщиком закодировали (enc) в h, декодировщиком разворачиваем в новый текст, это бутылочное горлышко. Придумали механизм внимания

![](./Materials/Lect9-1763030391975.jpeg)

Как усилить архитектуру? 
- Мы почему-то читаем текст слева направа
- Есть варики ...

Кодировщик
- Начнём с качественного прочтения входного текста
- Попробуем обогатить каждое входное слово информацией обо всем тексте
- Назовём это вниманием на себя (self-attention)

При этом векторы можно складывать, вычитать и т.д.

Это механизм обогащения слова контекстом

![](./Materials/Lect9-1763030534254.jpeg)

- Мы подмешиваем к слову t информацию из слова i на основе сходства этих слов
- Наверное, мы хотим смешивать информацию более хитро - например, смотреть на слова той же части речи или той же ...

![](./Materials/Lect9-1763030653641.jpeg)

Благодаря нормировке у нас не будет расти масштаб вектора слов

![](./Materials/Lect9-1763030823713.jpeg)

![](./Materials/Lect9-1763030834108.jpeg)

![](./Materials/Lect9-1763030839024.jpeg)

По аналогии - это одна свёртка с одним фильтром

мы выявляем одно свойство с одним ключом

![](./Materials/Lect9-1763031066289.jpeg)

![](./Materials/Lect9-1763031082468.jpeg)

тетта - матрицы для qkv, обучаемый параметр

Давайте сделаем несколько SA в параллели

![](./Materials/Lect9-1763031138959.jpeg)

![](./Materials/Lect9-1763031297137.jpeg)

![](./Materials/Lect9-1763031381481.jpeg)

левый блок - кодировщик трансформера (трансформер)

Mixture of experts - MoE
![](./Materials/Lect9-1763031761169.jpeg)

МИНУС: Информация про позицию всегда отсутствует

![](./Materials/Lect9-1763032006113.jpeg)

![](./Materials/Lect9-1763032080498.jpeg)

![](./Materials/Lect9-1763032085765.jpeg)

Еще более модная тема-  внутрь self attention добавить RoPE

![](./Materials/Lect9-1763032323815.jpeg)

![](./Materials/Lect9-1763032413907.jpeg)

по сути механизм внимания на другой текст

![](./Materials/Lect9-1763032637595.jpeg)

![](./Materials/Lect9-1763032689557.jpeg)


![](./Materials/Lect9-1763032972735.jpeg)

![](./Materials/Lect9-1763033067809.jpeg)

BERT: предобучение

![](./Materials/Lect9-1763033160656.jpeg)

это архитектура, которая хорошо подходит для классификации текстов

![](./Materials/Lect9-1763033343005.jpeg)

![](./Materials/Lect9-1763033348482.jpeg)

![](./Materials/Lect9-1763033468154.jpeg)

![](./Materials/Lect9-1763033723610.jpeg)

