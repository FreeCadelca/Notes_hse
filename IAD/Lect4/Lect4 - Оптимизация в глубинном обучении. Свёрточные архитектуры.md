Метод инерции (моментума)
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758794692737.jpeg|563x290]]

Момент Нестерова
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758794736486.jpeg]]

Мы впервые "шатнём" 8 признак на 7 шаге

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758794965572.jpeg]]

И к моменту, когда мы дойдём до 7 признака, шаг будет уже маленький

По разным параметрам мы движемся с разной скоростью
Будет здорово сделать адаптивную скорость

Допустим, признаки имеют разный масштаб - от единиц до миллионов. Тогда странно шагать по каждому параметру с одинаковой скоростью

Решение - AdaGrad
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758795127292.jpeg]]

давайте будет вспомогательная переменная G_j^t (j-ый параметр, шаг t).

j-ая координата - на сколько я модифицирую параметр на данном шаге

чем больше G для параметра - тем больше обновлений сделали

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758795221374.jpeg|417x138]]

Но AdaGrad может очень быстро затухать. Нужно починить это

RMSProp решает это (модификация AdaGrad) - добавляется коэф \alpha , который позволяет "забывать" прошлые шаги

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758795503107.jpeg]]

Adam - метод, совмещающий идеи метода инерции и AdaGrad

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758795618976.jpeg]]

Вместо обычного градиента - градиенты, накопленные с прошлых шагов, а также разделение скорости для разных переменных

---

Мы говорили, что регуляризация - важная штука

С более сложными моделями регуляризация нужна, но ее недостаточно. Нужно что-то, что еще сильнее упрощает ландшафт функционала ошибки

Dropout

Идея следующая:
Представим, что я хочу научить вас чему-то любой ценой
Есть разные технологии обучения. Жесткие блокирующие контрольные, можно еще чего-нибудь, а можно будить посреди ночи и спрашивать нужный материал. Тогда это будет буквально отпечатано в мозгах (жестоко). Если мы умеем решать что-то, когда мозг работает на половину, то на 100% мы точно будем помнить. Давайте тогда будем "мешать" обучаться модели. 

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796164096.jpeg|308x371]]

1 итерация обучения: удаляем один нейрон
2 итерация:
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796182146.jpeg]]

3 итерация: удаляем 2 нейрона
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796186099.jpeg]]

Dropout - можно определить, как слой d(x)

Параметров нет, единственный гиперпараметр - p (вероятность удаления нейрона)

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796362580.jpeg]]

пусть все веса единичные, тогда нужно компенсировать снижение масштаба суммы выходов
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796370572.jpeg]]

надо компенсировать снижение масштаба суммы выходов

на этапе обучения одни масштабы, после - другие, тогда делаем нормировку (1/(1-p) * m * x) (m - маска)
на этапе применения: d(x) = x

в ориг статье нет нормировки на этапе обучнеия, но есть домножение на (1-p) на этапе применения

вариант на слайде - inverted dropuot (чуть меньше операций во время применения сети)

Интерпретация: мы обучаем все возможные архитектуры нейросетей, которые получаются из исходной путем исключения нескольких нейронов

у всех одинаковые веса...


---

## Нормализация

Covariate shift
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796923870.jpeg]]

данные со временем меняются (и их распределение)

есть разные идеи, как с этим бороться 
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758796988762.jpeg]]

проблема с domain shift сильна в нейросетях, даже более острая

в какой-то момент огромный градиент на каком-то слое
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758797345551.jpeg]]

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758797623703.jpeg]]
на каком-то шаге град спуска может быть огромная производная, которая перефигачет все веса

а последующие уже настроены на другие веса (единички)

возникла идея это починить

давайте мы просто будем насильно сохранять масштаб на выходе данного слоя. Оно гарантирует, что масштаб будет около единичек

Batch Normalization
- реализуется как отдельный слой
- вычисляется для текущего батча
- оценим среднее и дисперсию каждой компоненты входного вектора


считаем по каждой координате среднее стандартное отклонение
каждый объект описывается m признаками, считаем дисперсию $\sigma$ 
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758797892835.jpeg]]

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758798057726.jpeg]]

RELU все отрицательные входы зануляет, поэтому она половину весов занулит, а потом и разрежет

давайте позволим модели выбирать $\beta$ и $\gamma$ , они будут обучаться
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758798301200.jpeg|262x134]]

![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758798339969.jpeg]]

во время применения нейронной сети:
- те же самые формулы, но вместо $\mu_B$ и $\sigma_B^2$ используем их средние значения по всем батчам

- обычно batch norm вставляется между полносвязными/сверточным слоем и нелинейностью
- позволяет увеличить длинну шага в град спуске
- не факт, что действительно устраняет covariance shift

а правда ли batch norm устраняет cov shift?
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758798681847.jpeg|480x291]]
Мы придумали что-то классное но не из неправильных предпосылок. Если cov shift присутствует, то все нормально, но что же он тогда решает?

Как связаны градиенты до и после обновления на предыдущих слоях?
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758798881592.jpeg]]

разницы на начальных слоях нету

а в dln даже хуже

В чем же польза от BatchNorm

**Функционал ошибки становится более "гладким"!**
![[Lect4 - Оптимизация в глубинном обучении. Свёрточные архитектуры-1758799068122.jpeg]]
