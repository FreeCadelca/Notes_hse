Переход между девайсами занимает время

> y.backward() -> считает производные выхода по входу

torchviz - чтобы рисовать деревья
![[Materials/Pasted image 20250911134758.jpeg]]

> x.grad - градиент тензора

> requires_grad - флаг для торча, если нам нужно хранить производные

градиенты сохраняются и копятся (суммируются), поэтому новый grad будет содержать сумму со старым. Возможно придется обнулять их (.grad.zero_())

мы сами пишем mse, чтобы вычислять граф вычислений

отвязка от графа через .item() - нужно для того, чтобы не перезабить память и занимать фиксированное место

.detach()

item - detach для константы, скаляра
detach универсальный, оставляет в пространстве тензоров

> torch.cuda.empty_cache()

***

Учим первую нейросеть

для этого нужно 3 вещи:
- обработка и поставка данных
- задавать нейросеть
- написать цикл обучения

torch.utils.data Dataset, DataLoader, transforms
Dataset - умеет давать по индексу
Dataloader - тонкие настройки
transforms - манипуляции с данными, трансформации с картинками

train и test выборки MNIST из готовых репозиториев
> аугментация данных

softmax - для детекции классов

логиты - с какой вероятностью тот или иной класс

по умолчанию require grads везде стоит во всех слоях

with torch.no_grad() - чтобы не забивалась память при ненужных градиентах

- можно включать детерменированное вычисление на cuda
- чекпоинты
- формат логов
- зафиксирование сида рандома
- 